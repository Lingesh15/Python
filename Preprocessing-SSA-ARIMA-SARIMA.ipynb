{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["PreProcessing Time_Series DataSet."],"metadata":{"id":"qEWAFTFTX5Py"}},{"cell_type":"code","source":["# Importing necessary libraries\n","import numpy as np\n","import pandas as pd\n","\n","# Load the dataset\n","# Assuming you have a CSV file named 'time_series_data.csv' with columns 'Date', 'Time', and 'Power'\n","data = pd.read_csv('time_series_data.csv')\n","\n","# Combine date and time columns into a single datetime column\n","data['DateTime'] = pd.to_datetime(data['Date'] + ' ' + data['Time'])\n","\n","# Drop the original 'Date' and 'Time' columns\n","data.drop(['Date', 'Time'], axis=1, inplace=True)\n","\n","# Set 'DateTime' column as the index\n","data.set_index('DateTime', inplace=True)\n","\n","# Sort the index in ascending order (if not already sorted)\n","data = data.sort_index()\n","\n","# Handling Missing Values\n","# Check for missing values\n","missing_values = data.isnull().sum()\n","print(\"Missing Values:\")\n","print(missing_values)\n","\n","# Fill missing values using forward fill (replace NaN values with the last valid observation)\n","data = data.fillna(method='ffill')\n","\n","# Resampling\n","# Resample the data to a daily frequency ('D') and compute the sum of power consumption for each day\n","data_resampled_daily = data.resample('D').sum()\n","\n","# Resample the data to a monthly frequency ('M') and compute the mean of power consumption for each month\n","data_resampled_monthly = data.resample('M').mean()\n","\n","# Scaling\n","# Apply Min-Max scaling to the power consumption values\n","from sklearn.preprocessing import MinMaxScaler\n","scaler = MinMaxScaler()\n","data_scaled = scaler.fit_transform(data)\n","\n","# Convert the scaled data back to a DataFrame\n","data_scaled = pd.DataFrame(data_scaled, columns=data.columns, index=data.index)\n","\n","# Plot the original and preprocessed data\n","import matplotlib.pyplot as plt\n","\n","plt.figure(figsize=(15, 6))\n","plt.plot(data.index, data['Power'], label='Original Data')\n","plt.title('Original Power Consumption Data')\n","plt.xlabel('DateTime')\n","plt.ylabel('Power (kWh)')\n","plt.legend()\n","plt.show()\n","\n","plt.figure(figsize=(15, 6))\n","plt.plot(data_resampled_daily.index, data_resampled_daily['Power'], label='Daily Resampled Data')\n","plt.title('Resampled Daily Power Consumption Data')\n","plt.xlabel('DateTime')\n","plt.ylabel('Power (kWh)')\n","plt.legend()\n","plt.show()\n","\n","plt.figure(figsize=(15, 6))\n","plt.plot(data_resampled_monthly.index, data_resampled_monthly['Power'], label='Monthly Resampled Data')\n","plt.title('Resampled Monthly Power Consumption Data')\n","plt.xlabel('DateTime')\n","plt.ylabel('Power (kWh)')\n","plt.legend()\n","plt.show()\n","\n","plt.figure(figsize=(15, 6))\n","plt.plot(data_scaled.index, data_scaled['Power'], label='Scaled Data')\n","plt.title('Scaled Power Consumption Data')\n","plt.xlabel('DateTime')\n","plt.ylabel('Scaled Power')\n","plt.legend()\n","plt.show()\n"],"metadata":{"id":"fkw532ICX7Aq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Detailed python code (.ipynb) for singular spectrum analysis of two months electricity consumption data having power in KWh to forecast the next month's power consumption. Including preprocessing the data to fit the model. Show accuracy of the forecast with respect to the original Third month's power consumption data"],"metadata":{"id":"QI3yO8bFZEna"}},{"cell_type":"code","source":["# Importing necessary libraries\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import mean_squared_error\n","from sklearn.preprocessing import MinMaxScaler\n","\n","# Singular Spectrum Analysis implementation\n","def ssa_forecast(series, window_size, forecast_steps):\n","    def embed_series(series, window_size):\n","        N = len(series)\n","        K = N - window_size + 1\n","        embedded = np.zeros((window_size, K))\n","        for i in range(K):\n","            embedded[:, i] = series[i:i+window_size]\n","        return embedded\n","\n","    def forecast_step(X, last_k):\n","        weights = X @ last_k\n","        return np.sum(weights * last_k[:, -1])\n","\n","    def forecast(X, window_size, forecast_steps):\n","        last_k = X[:, -window_size:]\n","        forecasted_values = np.zeros(forecast_steps)\n","        for i in range(forecast_steps):\n","            forecasted_values[i] = forecast_step(X, last_k)\n","            last_k = np.roll(last_k, -1, axis=1)\n","            last_k[:, -1] = forecasted_values[i]\n","        return forecasted_values\n","\n","    X = embed_series(series, window_size)\n","    forecast_values = forecast(X, window_size, forecast_steps)\n","    return forecast_values\n","\n","# Load the data\n","# Assuming you have a CSV file named 'electricity_data.csv' with columns 'Date' and 'Power' for the two months' data\n","data = pd.read_csv('electricity_data.csv')\n","# Assuming 'Date' column is in datetime format, if not, you can convert it using:\n","# data['Date'] = pd.to_datetime(data['Date'])\n","\n","# Preprocess the data\n","# Assuming the data is already preprocessed and is in the format needed for SSA.\n","# If not, you might need to perform some preprocessing steps like imputation of missing values, etc.\n","\n","# Split the data into two months\n","first_month_data = data.loc[data['Date'].dt.month == 1]['Power'].values\n","second_month_data = data.loc[data['Date'].dt.month == 2]['Power'].values\n","\n","# Concatenate two months data\n","two_months_data = np.concatenate([first_month_data, second_month_data])\n","\n","# Apply MinMaxScaler to scale the data\n","scaler = MinMaxScaler()\n","scaled_data = scaler.fit_transform(two_months_data.reshape(-1, 1)).flatten()\n","\n","# Singular Spectrum Analysis (SSA)\n","window_size = 30  # Choose an appropriate window size\n","forecast_steps = len(second_month_data)  # Forecast for the length of the second month\n","forecasted_values = ssa_forecast(scaled_data, window_size, forecast_steps)\n","\n","# Inverse transform to get the forecasted values in original scale\n","forecasted_values = scaler.inverse_transform(forecasted_values.reshape(-1, 1)).flatten()\n","\n","# Plotting the original and forecasted data\n","plt.figure(figsize=(10, 6))\n","plt.plot(np.arange(len(two_months_data)), two_months_data, label='Original Data')\n","plt.plot(np.arange(len(first_month_data), len(two_months_data) + forecast_steps), forecasted_values, label='Forecasted Data')\n","plt.xlabel('Time')\n","plt.ylabel('Power Consumption (KWh)')\n","plt.title('Electricity Consumption Forecast')\n","plt.legend()\n","plt.show()\n","\n","# Evaluate forecast accuracy\n","third_month_data = data.loc[data['Date'].dt.month == 3]['Power'].values\n","mse = mean_squared_error(third_month_data, forecasted_values[:len(third_month_data)])\n","print(f\"Mean Squared Error (MSE) between forecast and actual third month's data: {mse}\")\n"],"metadata":{"id":"Lp_BxhFBZF-i"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[" .ipynb code to preprocess a time series dataset with date [dd-mm-yyyy], time[hr-mins-seconds] and power consumed in Kwh. Write the code with all possible preprocessing techniques, scenarios and resampling. The preprocessed dataset should be used for Singular Spectrum Analysis model, ARIMA,SARIMA to forecast next month's power consumption BASED ON LAST  TWO MONTH'S  POWER CONSUMPTION."],"metadata":{"id":"DL0a3j6YZgTy"}},{"cell_type":"code","source":["# Importing necessary libraries\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from statsmodels.tsa.seasonal import seasonal_decompose\n","from statsmodels.tsa.statespace.sarimax import SARIMAX\n","from statsmodels.tsa.arima.model import ARIMA\n","from sklearn.metrics import mean_squared_error\n","from sklearn.preprocessing import MinMaxScaler\n","\n","# Load the dataset\n","data = pd.read_csv('time_series_data.csv')\n","\n","# Preprocessing the dataset\n","# Assuming 'Date' and 'Time' columns are in datetime format, if not, you can convert them using:\n","# data['DateTime'] = pd.to_datetime(data['Date'] + ' ' + data['Time'])\n","# Set 'DateTime' column as the index\n","data['DateTime'] = pd.to_datetime(data['Date'] + ' ' + data['Time'])\n","data.set_index('DateTime', inplace=True)\n","# Drop 'Date' and 'Time' columns as they are no longer needed\n","data.drop(['Date', 'Time'], axis=1, inplace=True)\n","\n","# Resample the data to hourly frequency\n","data_hourly = data.resample('H').sum()\n","\n","# Fill missing values using forward fill method\n","data_hourly.fillna(method='ffill', inplace=True)\n","\n","# Plot the resampled data\n","plt.figure(figsize=(10, 6))\n","plt.plot(data_hourly.index, data_hourly['Power'], label='Resampled Data')\n","plt.xlabel('Date')\n","plt.ylabel('Power Consumption (KWh)')\n","plt.title('Resampled Time Series Data')\n","plt.legend()\n","plt.show()\n","\n","# Singular Spectrum Analysis (SSA)\n","# Implement SSA to decompose the time series into components\n","# You can use the previously provided SSA implementation\n","\n","# ARIMA model\n","# Split the data into training and testing sets\n","train_size = int(len(data_hourly) * 0.8)\n","train, test = data_hourly.iloc[:train_size], data_hourly.iloc[train_size:]\n","\n","# Fit ARIMA model\n","model_arima = ARIMA(train, order=(5,1,0))\n","arima_results = model_arima.fit()\n","\n","# Forecast using ARIMA model\n","arima_forecast = arima_results.forecast(steps=len(test))[0]\n","\n","# SARIMA model\n","# Fit SARIMA model\n","model_sarima = SARIMAX(train, order=(1, 1, 1), seasonal_order=(1, 1, 1, 24))\n","sarima_results = model_sarima.fit()\n","\n","# Forecast using SARIMA model\n","sarima_forecast = sarima_results.forecast(steps=len(test))\n","\n","# Evaluate forecast accuracy\n","mse_arima = mean_squared_error(test, arima_forecast)\n","mse_sarima = mean_squared_error(test, sarima_forecast)\n","print(f\"MSE for ARIMA: {mse_arima}\")\n","print(f\"MSE for SARIMA: {mse_sarima}\")\n"],"metadata":{"id":"17AvDoSnZuHT"},"execution_count":null,"outputs":[]}]}